{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":125192,"databundleVersionId":15408205},{"sourceType":"datasetVersion","sourceId":14919407,"datasetId":9546247,"databundleVersionId":15785871},{"sourceType":"datasetVersion","sourceId":14951390,"datasetId":9569164,"databundleVersionId":15821647}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport optuna\nimport random\nimport json\nimport warnings\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom copy import deepcopy\n\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import RobustScaler, KBinsDiscretizer, FunctionTransformer\nfrom sklearn.ensemble import HistGradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom category_encoders import TargetEncoder\n\nimport xgboost as xgboost\nimport lightgbm as lightgbm\nfrom catboost import CatBoostClassifier\nfrom sklearn import set_config\n\n\nset_config(transform_output=\"pandas\")\nseed = 2026\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T00:38:31.926567Z","iopub.execute_input":"2026-02-25T00:38:31.926755Z","iopub.status.idle":"2026-02-25T00:38:41.269720Z","shell.execute_reply.started":"2026-02-25T00:38:31.926734Z","shell.execute_reply":"2026-02-25T00:38:41.269114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = '/kaggle/input/playground-series-s6e2'\ntrain_data = pd.read_csv(os.path.join(base_path, 'train.csv'), index_col='id')\ntest_data = pd.read_csv(os.path.join(base_path, 'test.csv'), index_col='id')\nsubmission = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))\ntarget_col = 'Heart Disease'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T00:38:41.271209Z","iopub.execute_input":"2026-02-25T00:38:41.271776Z","iopub.status.idle":"2026-02-25T00:38:42.233980Z","shell.execute_reply.started":"2026-02-25T00:38:41.271751Z","shell.execute_reply":"2026-02-25T00:38:42.233371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T05:17:19.274748Z","iopub.execute_input":"2026-02-24T05:17:19.275417Z","iopub.status.idle":"2026-02-24T05:17:19.285621Z","shell.execute_reply.started":"2026-02-24T05:17:19.275389Z","shell.execute_reply":"2026-02-24T05:17:19.284800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()\nprint('-'*80, end='\\n\\n')\n\ncat_col_list = []\nnom_col_list = []\n\nfor _col in train_data.columns:\n    if _col == target_col:\n        continue\n    if train_data[_col].dtype == int:\n        if train_data[_col].nunique() < 20:\n            cat_col_list.append(_col)\n            continue\n    nom_col_list.append(_col)\n\ntrain_data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:10:05.898588Z","iopub.execute_input":"2026-02-24T06:10:05.898850Z","iopub.status.idle":"2026-02-24T06:10:06.221858Z","shell.execute_reply.started":"2026-02-24T06:10:05.898826Z","shell.execute_reply":"2026-02-24T06:10:06.221115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# categorical variable pie chart\nsns.set_theme(style=\"whitegrid\")\ncolors = sns.color_palette('pastel')\nfig, axes = plt.subplots(math.ceil(len(cat_col_list)/3), 3, figsize=(18, 12))\naxes = axes.flatten()\n\nfor _i, _col in enumerate(cat_col_list):\n    sample_data = train_data[_col].value_counts(normalize=True)\n    axes[_i].pie(sample_data, labels=sample_data.index, colors=colors[:sample_data.__len__()], autopct='%.0f%%', startangle=120)\n    axes[_i].set_title(f'{_col} Pie Chart')\nfor j in range(len(cat_col_list), len(axes)):\n    axes[j].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T05:39:16.778416Z","iopub.execute_input":"2026-02-23T05:39:16.778755Z","iopub.status.idle":"2026-02-23T05:39:17.530714Z","shell.execute_reply.started":"2026-02-23T05:39:16.778730Z","shell.execute_reply":"2026-02-23T05:39:17.529892Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# numerical variable kde-box plot\ncolors = sns.color_palette('pastel')\n\n# train_data[nom_col_list].hist(bins=100, figsize=(16,12))\nfor _col in nom_col_list:\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6), gridspec_kw={'width_ratios': [2, 1]})\n    sns.kdeplot(\n        data=train_data, x=_col, ax=axes[0], fill=True)\n    mid = train_data[_col].quantile(.5)\n    axes[0].set_title(f'{_col} Distribution (KDE)', fontsize=12)\n    axes[0].axvline(x=mid, color='red', linestyle='--', linewidth=1, label='midian')\n    \n    sns.boxplot(\n        data=train_data, y=_col, ax=axes[1],\n        orient='v', width=0.5, linewidth=1, fliersize=3)\n    axes[1].set_title(f'{_col} Boxplot', fontsize=12)\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T05:39:17.531745Z","iopub.execute_input":"2026-02-23T05:39:17.532027Z","iopub.status.idle":"2026-02-23T05:39:35.620499Z","shell.execute_reply.started":"2026-02-23T05:39:17.531997Z","shell.execute_reply":"2026-02-23T05:39:35.619795Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# correlation_matrix\ncorrelation_matrix = train_data[cat_col_list + nom_col_list].corr()\nplt.figure(figsize=(18, 10)) \nsns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt=\".2f\")\nplt.title('Correlation Matrix of Features', fontsize=16)\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T05:39:35.621383Z","iopub.execute_input":"2026-02-23T05:39:35.621700Z","iopub.status.idle":"2026-02-23T05:39:36.561244Z","shell.execute_reply.started":"2026-02-23T05:39:35.621661Z","shell.execute_reply":"2026-02-23T05:39:36.560444Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# categorical variable pie chart with target\nfor _i, _col in enumerate(cat_col_list):\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6), gridspec_kw={'width_ratios': [1, 1]})\n    \n    sample_data = train_data[[_col,target_col]].groupby(target_col).value_counts(normalize=True)\n    axes[0].pie(sample_data['Absence'], labels=sample_data['Absence'].index, colors=colors[:sample_data['Absence'].__len__()], autopct='%.0f%%', startangle=120)\n    axes[0].set_title(f'{_col}-Absence Pie Chart')\n    \n    axes[1].pie(sample_data['Presence'], labels=sample_data['Presence'].index, colors=colors[:sample_data['Presence'].__len__()], autopct='%.0f%%', startangle=120)\n    axes[1].set_title(f'{_col}-Presence Pie Chart')\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T05:39:36.562389Z","iopub.execute_input":"2026-02-23T05:39:36.562838Z","iopub.status.idle":"2026-02-23T05:39:38.544635Z","shell.execute_reply.started":"2026-02-23T05:39:36.562804Z","shell.execute_reply":"2026-02-23T05:39:38.543784Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# numerical variable kde-box plot with target\ncolors = sns.color_palette('pastel')\n\n# train_data[nom_col_list].hist(bins=100, figsize=(16,12))\nfor _col in nom_col_list:\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6), gridspec_kw={'width_ratios': [2, 1]})\n    sns.kdeplot(\n        data=train_data, x=_col, hue=target_col, ax=axes[0], fill=True, palette=colors[:2])\n    mid = train_data[[_col, target_col]].groupby(target_col).quantile(.5).to_dict()[_col]\n    axes[0].set_title(f'{_col} Distribution (KDE)', fontsize=12,)\n    axes[0].axvline(x=mid['Presence'], color='blue', linestyle='--', linewidth=1, label='Presence_midian')\n    axes[0].axvline(x=mid['Absence'], color='red', linestyle='--', linewidth=1, label='Absence_midian')\n    \n    sns.boxplot(\n        data=train_data, x=target_col, y=_col, hue=target_col, ax=axes[1], palette=colors[:2],\n        orient='v', width=0.5, linewidth=1, fliersize=3, legend=False)\n    axes[1].set_title(f'{_col} Boxplot', fontsize=12)\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T09:21:37.496341Z","iopub.execute_input":"2026-02-09T09:21:37.496866Z","iopub.status.idle":"2026-02-09T09:22:06.205151Z","shell.execute_reply.started":"2026-02-09T09:21:37.496828Z","shell.execute_reply":"2026-02-09T09:22:06.204221Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_enc_col = ['Number of vessels fluro', 'Thallium', 'Chest pain type']\n\ntrain_data[target_col] = LabelEncoder().fit_transform(train_data[target_col]).astype(np.uint8)\nglobal_stats = {'mean': train_data[target_col].mean(), 'median': train_data[target_col].median(), 'count': 0}\n\n\nX = train_data.iloc[:, :-1]\ny = train_data.pop(target_col)\n\nfor col in target_enc_col:\n    X[col] = X[col].astype(str)\n    test_data[col] = test_data[col].astype(str)\n\nclass FrequencyEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.freq_maps = {}\n        self.global_mean_freq = None\n        self._feature_names = None \n\n    def set_output(self, *, transform=None):\n        if transform is None or transform in [\"pandas\", \"default\"]:\n            return self\n        raise ValueError(f\"Unsupported transform: {transform}\")\n\n    def fit(self, X, y=None):\n        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n        self._feature_names = list(X_df.columns)\n        self.global_mean_freq = 0.0 \n        for col in self._feature_names: \n            freqs = X_df[col].value_counts(normalize=True).to_dict()\n            self.freq_maps[col] = freqs\n        return self\n\n    def transform(self, X, y=None):\n        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self._feature_names)\n        X_encoded = pd.DataFrame()\n        for col in X_df.columns:\n            mapping = self.freq_maps.get(col, {})\n            X_encoded[col] = X_df[col].map(mapping).fillna(self.global_mean_freq).astype(float)\n        X_encoded.columns = [f'frq_enc__{c}' for c in X_encoded.columns] \n        return X_encoded\n\ndef target_stats(X, features, st_type, global_stats=global_stats):\n    X_stat = pd.DataFrame()\n    for c in features:\n        c_st_dict = X.groupby(c)[target_col].agg([st_type]).to_dict()[st_type]\n        X_stat[c] = X[c].map(c_st_dict).fillna(global_stats[st_type])\n    return X_stat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:10:06.222867Z","iopub.execute_input":"2026-02-24T06:10:06.223127Z","iopub.status.idle":"2026-02-24T06:10:06.919030Z","shell.execute_reply.started":"2026-02-24T06:10:06.223107Z","shell.execute_reply":"2026-02-24T06:10:06.918433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"minmax_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('robust_scaling', RobustScaler())]\n)\nordinal_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"most_frequent\")),\n     ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=np.int8))]\n)\nkbins_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('kbins', KBinsDiscretizer(n_bins=10, strategy='uniform', encode='ordinal', random_state=seed))]\n)\nfreq_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n    ('freq', FrequencyEncoder())]\n)\nlog_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('log_trans', FunctionTransformer(func=lambda x: np.log(x + 0.001), feature_names_out='one-to-one')),\n     ('robust_scaling', RobustScaler())]\n)\nsquare_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('square_trans', FunctionTransformer(func=np.square, feature_names_out='one-to-one')),\n     ('robust_scaling', RobustScaler())]\n)\ncube_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('cube_trans', FunctionTransformer(func=lambda x: np.power(x, 3), feature_names_out='one-to-one')),\n     ('robust_scaling', RobustScaler())]\n)\nsqrt_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('sqrt_trans', FunctionTransformer(func=np.sqrt, feature_names_out='one-to-one')),\n     ('robust_scaling', RobustScaler())]\n)\ncbrt_pipeline = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('cbrt_trans', FunctionTransformer(func=np.cbrt, feature_names_out='one-to-one')),\n     ('robust_scaling', RobustScaler())]\n)\nmedian_pipeline_placeholder = Pipeline(\n    [('imputer', SimpleImputer(strategy=\"median\")),\n     ('median_enc', FunctionTransformer(func=lambda x, features, stats, global_stats: target_stats(x, features, stats, 'median', global_stats), validate=False, feature_names_out='one-to-one'))]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:10:06.919872Z","iopub.execute_input":"2026-02-24T06:10:06.920119Z","iopub.status.idle":"2026-02-24T06:10:06.928034Z","shell.execute_reply.started":"2026-02-24T06:10:06.920097Z","shell.execute_reply":"2026-02-24T06:10:06.927327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessor = ColumnTransformer([\n    (\"minmax\", minmax_pipeline, nom_col_list),\n    (\"cluster\", kbins_pipeline, nom_col_list),\n    (\"frqn\", freq_pipeline, nom_col_list),\n    (\"ordinal\", ordinal_pipeline, cat_col_list),\n    (\"frqc\", freq_pipeline, cat_col_list),\n    (\"log\", log_pipeline, nom_col_list),\n    (\"square\", square_pipeline, nom_col_list),\n    (\"target_enc\", TargetEncoder(smoothing=40, min_samples_leaf=20), target_enc_col),\n    (\"raw_age\", FunctionTransformer(lambda x: x), [\"Age\"]),\n], remainder='passthrough')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:10:06.928931Z","iopub.execute_input":"2026-02-24T06:10:06.929265Z","iopub.status.idle":"2026-02-24T06:10:06.941599Z","shell.execute_reply.started":"2026-02-24T06:10:06.929236Z","shell.execute_reply":"2026-02-24T06:10:06.940964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_interactions(X):\n    X = X.copy()\n    # 1. RPP (혈압 * 심박수)\n    X['HR_times_BP'] = X['minmax__Max HR'] * X['minmax__BP']\n    \n    # 2. 연령 대비 심박 비율\n    X['HR_Age_Ratio'] = X['minmax__Max HR'] / (220 - X['raw_age__Age'] + 1e-6)\n    \n    # 3. HR & Cholesterol\n    X['HR_Chol_Mul'] = X['minmax__Max HR'] * X['minmax__Cholesterol']\n    \n    # 4. ST_depression & Chest_pain\n    X['ST_Chest_Interaction'] = X['minmax__ST depression'] * X['ordinal__Chest pain type']\n    # 5. ST depression & Max HR\n    X['ST_HR_times'] = X['minmax__ST depression'] * X['minmax__Max HR']\n    # 혈압과 심박수 비율\n    X['HR_BP_Ratio'] = X['minmax__Max HR'] / (X['minmax__BP'] + 1e-6)\n    return X\n\n# 파이프라인에 추가할 변환기\ninteraction_transformer = FunctionTransformer(create_interactions)\n\ndef feature_drop(X):\n    X = X.copy()\n    # lgbm 중요도에서 100이하 제거\n    low_importance_cols = [\n        'ordinal__FBS over 120', \n        'cluster__Age', 'cluster__BP', 'cluster__Cholesterol', 'cluster__ST depression', \n        'frqc__frq_enc__FBS over 120', 'frqc__frq_enc__Number of vessels_fluro', 'frqc__frq_enc__Thallium', 'frqc__frq_enc__Exercise angina', 'frqc__frq_enc__EKG results',\n    ]\n    existing_cols = [c for c in low_importance_cols if c in X.columns]\n    return X.drop(columns=existing_cols)\n\ndrop_transformer = FunctionTransformer(feature_drop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:12:52.495369Z","iopub.execute_input":"2026-02-24T06:12:52.496239Z","iopub.status.idle":"2026-02-24T06:12:52.502401Z","shell.execute_reply.started":"2026-02-24T06:12:52.496201Z","shell.execute_reply":"2026-02-24T06:12:52.501790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessing_base = Pipeline([\n    ('preprocessor', preprocessor),   \n    ('interactions', interaction_transformer), \n    ('lowdrop', drop_transformer),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T06:12:52.690821Z","iopub.execute_input":"2026-02-24T06:12:52.691567Z","iopub.status.idle":"2026-02-24T06:12:52.695403Z","shell.execute_reply.started":"2026-02-24T06:12:52.691543Z","shell.execute_reply":"2026-02-24T06:12:52.694814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model\nGPU_ACC = True\n\nlr = LogisticRegression()\net = ExtraTreesClassifier()\nhgbc = HistGradientBoostingClassifier()\nxgb = xgboost.XGBClassifier()\nlgbm = lightgbm.LGBMClassifier()\ncatc = CatBoostClassifier()\n\n# Model space\nEstimatorStr = {1: 'lr', 2: 'et', 3: 'hgbc', 4: 'xgb', 5: 'lgbm', 6: 'catc'}\nEstimatorMdl = {1: lr,   2: et,   3: hgbc,   4: xgb,   5: lgbm,   6: catc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T05:55:39.075969Z","iopub.execute_input":"2026-02-23T05:55:39.076729Z","iopub.status.idle":"2026-02-23T05:55:39.081658Z","shell.execute_reply.started":"2026-02-23T05:55:39.076697Z","shell.execute_reply":"2026-02-23T05:55:39.080855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## Tuned hyperparameter sets\ndef objective(trial):\n    if est_id == 1:\n        params = {\n            'C': trial.suggest_float('C', 1e-4, 10.0, log=True),\n            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n            'solver': 'saga',\n            'max_iter': 1000,\n            'tol': 0.001,\n            # 'l1_ratio': trial.suggest_float('lr_l1_ratio', 0, 1) if trial.params.get('lr_penalty') == 'elasticnet' else None\n        }\n    elif est_id == 2:\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n            'max_depth': trial.suggest_int('max_depth', 10, 50),\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n            'max_features': trial.suggest_float('max_features', 0.3, 1.0),\n            'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n        }\n    elif est_id == 3:\n        params = {\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'max_iter': trial.suggest_int('max_iter', 500, 2000),\n            'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 15, 127),\n            'max_depth': trial.suggest_int('max_depth', 5, 20),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100),\n            'l2_regularization': trial.suggest_float('l2_regularization', 1e-3, 10.0, log=True),\n            'early_stopping': True\n        }\n    elif est_id == 4:\n        params = {\n            'n_estimators': 1500,\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n            'reg_lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n            'reg_alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n            'early_stopping_rounds': 50,\n            'tree_method': 'hist', \n        }\n    elif est_id == 5:\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n            'num_leaves': trial.suggest_int('num_leaves', 31, 255),\n            'max_depth': trial.suggest_int('max_depth', 5, 10),\n            'min_child_samples': trial.suggest_int('min_child_samples', 100, 500),\n            'colsample_bytree': trial.suggest_float('feature_fraction', 0.4, 0.6),\n            'subsample': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n            'subsample_freq': trial.suggest_int('bagging_freq', 1, 7),\n            'early_stopping_round': 100,\n            'objective': 'binary',\n            'metric': 'auc', \n            'is_unbalance': True, \n            'random_state': seed, \n            'device': 'gpu' if GPU_ACC else 'cpu', \n            'verbosity': -1,\n            'importance_type': 'gain',\n        }\n    elif est_id == 6:\n        params = {\n            'iterations': 2000,\n            'depth': trial.suggest_int('depth', 4, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n            'random_strength': trial.suggest_float('random_strength', 1.0, 10.0),\n            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n            'boosting_type': 'Plain',\n            'bootstrap_type': 'MVS', # Bernoulli -> MVS 권장 (GPU 효율)\n            'early_stopping_rounds': 50,\n        }\n    model_param = params\n    cv_score = []\n\n    # skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=seed)\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.3, stratify=y, random_state=seed)\n\n    # for train_idx, val_idx in skf.split(X, y):\n    #     X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    #     y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n    model = deepcopy(EstimatorMdl[est_id])\n    pipeline_copy = deepcopy(preprocessing_base)\n    pipeline_copy.set_output(transform='pandas')\n        \n    X_tr = pipeline_copy.fit_transform(X_tr, y_tr)\n    X_val = pipeline_copy.transform(X_val)\n    if est_id == 6:\n        ordinal_columns = make_column_selector(pattern='ordinal|cluster')(X_tr)\n        X_tr[ordinal_columns] = X_tr[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'uint8')\n        X_val[ordinal_columns] = X_val[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'uint8')\n        params.update({'cat_features': ordinal_columns})\n    \n    model.set_params(**model_param)\n    if est_id in EST_IDS_W_EARLYSTOPPING:\n        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)])\n    else:\n        model.fit(X_tr, y_tr)\n    preds = model.predict_proba(X_val)[:, 1]   \n    return roc_auc_score(y_val, preds)\n    # preds = model.predict_proba(X_val)[:, 1]\n    # cv_score.append(roc_auc_score(y_val, preds))\n    # return np.mean(cv_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T15:32:57.041518Z","iopub.execute_input":"2026-02-18T15:32:57.042702Z","iopub.status.idle":"2026-02-18T15:32:57.061662Z","shell.execute_reply.started":"2026-02-18T15:32:57.042657Z","shell.execute_reply":"2026-02-18T15:32:57.060293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_params = {\n    \"C\": 0.4015730855842094,\n    \"penalty\": \"l1\",\n    'solver': 'saga',\n    'max_iter': 1000,\n    'tol': 0.001,\n    'random_state': seed, \n    'n_jobs': -1\n}\nhgbc_params = {\n    \"learning_rate\": 0.0427125950073068,\n    \"max_iter\": 1411,\n    \"max_leaf_nodes\": 39,\n    \"max_depth\": 6,\n    \"min_samples_leaf\": 12,\n    \"l2_regularization\": 0.0011130407805067782,\n    'scoring': 'roc_auc', \n    'class_weight': 'balanced', \n    'random_state': seed, \n    'early_stopping': True,\n}\nlgbm_params ={\n    \"n_estimators\": 3000,\n    \"learning_rate\": 0.012396577620934138,\n    \"num_leaves\": 158,\n    \"max_depth\": 5,\n    \"min_child_samples\": 148,\n    \"feature_fraction\": 0.4229243485116701,\n    \"bagging_fraction\": 0.7423977412122245,\n    \"bagging_freq\": 4,\n    'early_stopping_round': 100,\n    'objective': 'binary',\n    'metric': 'auc', \n    'is_unbalance': False, \n    'random_state': seed, \n    'device': 'gpu' if GPU_ACC else 'cpu', \n    'verbosity': -1,\n}\ncatc_params = {\n    'iterations': 3000,\n    \"depth\": 10,\n    \"learning_rate\": 0.0352884492889246,\n    \"l2_leaf_reg\": 3.282062804017597,\n    \"random_strength\": 7.611455519781875,\n    \"bagging_temperature\": 0.6862915363980234,\n    'boosting_type': 'Plain',\n    'bootstrap_type': 'MVS', # Bernoulli -> MVS 권장 (GPU 효율)\n    'early_stopping_rounds': 50,\n    'eval_metric': 'AUC', \n    'auto_class_weights': 'Balanced', \n    'random_state': seed, \n    'task_type': 'GPU' if GPU_ACC else 'CPU', \n    'verbose': False,\n}\nparams = {1: lr_params, 3: hgbc_params, 5: lgbm_params, 6: catc_params}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T05:40:32.974277Z","iopub.execute_input":"2026-02-23T05:40:32.975075Z","iopub.status.idle":"2026-02-23T05:40:32.981851Z","shell.execute_reply.started":"2026-02-23T05:40:32.975043Z","shell.execute_reply":"2026-02-23T05:40:32.981174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Find best params\nTUNING = True\nFOLDS = 7\n\nEST_IDS = [5] # {1: 'lr', 2: 'xtree', 3: 'hgbc', 4: 'xgb', 5: 'lgbm', 6: 'catc'}\nEST_IDS_W_EARLYSTOPPING = [4,5,6]\nEST_IDS_W_CAT_FEAT = [3, 4, 5, 6]\n\n\nfor est_id in EST_IDS:\n    pipeline = Pipeline([('est', EstimatorMdl[est_id])])\n    best_p = {}\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=seed)\n\n    if TUNING:\n        study = optuna.create_study(direction=\"maximize\")\n        study.optimize(objective, n_trials=30)\n\n        best_info = {\n            \"best_score\": study.best_value,\n            \"best_params\": study.best_params,\n            \"model_name\": f\"{EstimatorStr[est_id]}\" # 예시\n        }\n\n        with open(f\"best_config_{EstimatorStr[est_id]}.json\", \"w\") as f:\n            json.dump(best_info, f, indent=4)\n        \n        print(\"Best Parameters:\", study.best_params)\n        best_p = study.best_params\n        # model_name = best_p.pop('model_type')\n    \n    oof_preds = np.zeros(len(X))\n    # all_importances = []\n    final_test_preds = np.zeros(len(test_data))\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=seed)\n    \n    for _id, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        pipeline_copy = deepcopy(preprocessing_base)\n        pipeline_copy.set_output(transform='pandas')\n\n        X_tr = pipeline_copy.fit_transform(X_tr, y_tr)\n        X_val = pipeline_copy.transform(X_val)\n        test_scaled = pipeline_copy.transform(test_data)\n\n        if est_id == 6:\n            ordinal_columns = make_column_selector(pattern='ordinal|cluster')(X_tr)\n            X_tr[ordinal_columns] = X_tr[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'uint8')\n            X_val[ordinal_columns] = X_val[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'uint8')\n            test_scaled[ordinal_columns] = test_scaled[ordinal_columns].astype(str).astype('category' if est_id in EST_IDS_W_CAT_FEAT else 'uint8')\n            # params.update({'cat_features': ordinal_columns})\n        \n        # fold_model = LogisticRegression(solver= 'saga', max_iter=1000, tol=0.001, random_state=seed, n_jobs=-1, **best_p)\n        # fold_model = ExtraTreesClassifier(n_jobs=-1, random_state=seed, **best_p)\n        # fold_model = HistGradientBoostingClassifier(scoring='roc_auc', class_weight='balanced', random_state=seed, early_stopping=True, **best_p)\n        # fold_model = xgboost.XGBClassifier(objective='binary:logistic', enable_categorical=True, device='cuda' if GPU_ACC else 'cpu',random_state=seed, eval_metric=\"auc\", **best_p)\n        fold_model = lightgbm.LGBMClassifier(objective='binary', metric='auc', is_unbalance=False, random_state=seed, device ='gpu' if GPU_ACC else 'cpu', verbosity=-1, early_stopping_round= 50, importance_type='gain', **best_p)\n        # fold_model = CatBoostClassifier(eval_metric='AUC', auto_class_weights='Balanced', random_state=seed, task_type='GPU' if GPU_ACC else 'CPU', verbose=False, \n        #                                 boosting_type='Plain', bootstrap_type='MVS', # Bernoulli -> MVS 권장 (GPU 효율)\n        #                                 early_stopping_rounds=50, **best_p)\n        # fold_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), cat_features=ordinal_columns)\n        fold_model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n        # fold_model.fit(X_tr, y_tr)\n\n        val_preds = fold_model.predict(X_val)\n        oof_preds[val_idx] = val_preds\n        final_test_preds += fold_model.predict_proba(test_scaled)[:, 1] / skf.n_splits\n        \n        # scores = fold_model.get_booster().get_score(importance_type='gain') # xgb\n        \n        importance_df = pd.DataFrame({\n            'Feature': fold_model.feature_name_,\n            'Importance': fold_model.feature_importances_\n        }).sort_values(by='Importance', ascending=False)\n        importance_df.to_csv(f'importance_{_id}.csv') # lgbm\n        \n        # importance_df = pd.DataFrame({\n        #     'Feature': fold_model.feature_names_,\n        #     'Importance': fold_model.get_feature_importance()\n        # }).sort_values(by='Importance', ascending=False)\n        # importance_df.to_csv(f'importance_{_id}.csv') # catc\n        \n        preds = fold_model.predict_proba(X_val)[:, 1]   \n        fold_score = roc_auc_score(y_val, preds)\n        # fold_score = fold_model.score(X_val, y_val)\n        print(f\"Fold {_id}  Score: {fold_score:.4f}\")\n    \n    oof_accuracy = np.mean(oof_preds == y)\n    print(f\"\\n[최종 결과] OOF 전체 정확도: {oof_accuracy:.4f}\")\n    submission['Heart Disease'] = final_test_preds\n    submission.to_csv(f'submission_{EstimatorStr[est_id]}.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T15:32:59.989544Z","iopub.execute_input":"2026-02-18T15:32:59.989937Z","execution_failed":"2026-02-18T15:36:22.416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- lr: .94972\n\n- et: .95095\n\n- hgbc: .95340\n\n- xgb: .94851\n\n- lgbm: .95357\n\n- cat: .95253\n\n상위권 3개 hgbc, lgbm, cat를 기준으로 잡고, lr모델을 약간의 첨가하는 방식을 선택","metadata":{}},{"cell_type":"code","source":"# sub_folder = '/kaggle/input/datasets/domchitdomchit/submission'\n# sub_te_folder = '/kaggle/input/datasets/domchitdomchit/submission-te'\n# sub_file_list = os.listdir(sub_folder)\n# sub_te_file_list = os.listdir(sub_te_folder)\n# sub_dict = {}\n# for _file in sub_file_list:\n#     model_name = _file.split('.')[0][11:]\n#     sub_dict[model_name] = pd.read_csv(os.path.join(sub_folder, _file))['Heart Disease'].values\n# for _file in sub_te_file_list:\n#     if 'te' in _file:\n#         model_name = _file.split('.')[0][11:]\n#     else:\n#         model_name = _file.split('.')[0][11:] + '_te'\n#     sub_dict[model_name] = pd.read_csv(os.path.join(sub_te_folder, _file))['Heart Disease'].values\n\n\n# corr_matrix = pd.DataFrame(sub_dict).corr()\n# plt.figure(figsize=(10, 8))\n# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".4f\", linewidths=0.5)\n# plt.title('Model Prediction Correlation')\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T00:40:56.442140Z","iopub.execute_input":"2026-02-25T00:40:56.442699Z","iopub.status.idle":"2026-02-25T00:40:57.210883Z","shell.execute_reply.started":"2026-02-25T00:40:56.442672Z","shell.execute_reply":"2026-02-25T00:40:57.210242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from scipy.stats import rankdata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T00:41:39.220041Z","iopub.execute_input":"2026-02-25T00:41:39.220809Z","iopub.status.idle":"2026-02-25T00:41:39.224381Z","shell.execute_reply.started":"2026-02-25T00:41:39.220777Z","shell.execute_reply":"2026-02-25T00:41:39.223674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# new_lgbm = rankdata(sub_dict['lgbm'])/(len(sub_dict['lgbm']) + 1)\n# new_lgbm_te = rankdata(sub_dict['lgbm_te'])/(len(sub_dict['lgbm_te']) + 1)\n# new_cat_te = rankdata(sub_dict['cat_te'])/(len(sub_dict['cat_te']) + 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T00:43:27.536711Z","iopub.execute_input":"2026-02-25T00:43:27.537247Z","iopub.status.idle":"2026-02-25T00:43:27.559951Z","shell.execute_reply.started":"2026-02-25T00:43:27.537220Z","shell.execute_reply":"2026-02-25T00:43:27.559368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# final_rank_blend = (new_lgbm_te * 0.5) + (new_lgbm * 0.5)\n# submission['Heart Disease'] = final_rank_blend\n# submission.to_csv(f'submission_sub_Rb_lgbm.csv', index=False)\n\n# final_rank_blend = (new_lgbm_te * 0.45) + (new_lgbm * 0.5) + (new_cat_te * 0.05)\n# submission['Heart Disease'] = final_rank_blend\n# submission.to_csv(f'submission_sub_Rb_.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FOLDS = 7\n# EST_IDS = [5] # {1: 'lr', 2: 'xtree', 3: 'hgbc', 4: 'xgb', 5: 'lgbm', 6: 'catc'}\n# oof_preds = dict()\n# final_test_preds = dict()\n\n# for est_id in EST_IDS:\n#     oof_preds[est_id] = np.zeros(len(X))\n#     final_test_preds[est_id] = np.zeros(len(test_data))\n#     skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=seed)\n#     for _id, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n#         X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#         y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n#         pipeline_copy = deepcopy(preprocessing_base)\n#         pipeline_copy.set_output(transform='pandas')\n    \n#         X_tr = pipeline_copy.fit_transform(X_tr, y_tr)\n#         X_val = pipeline_copy.transform(X_val)\n#         test_scaled = pipeline_copy.transform(test_data)\n    \n#         if est_id == 6:\n#             ordinal_columns = make_column_selector(pattern='ordinal|cluster')(X_tr)\n#             X_tr[ordinal_columns] = X_tr[ordinal_columns].astype(str).astype('category')\n#             X_val[ordinal_columns] = X_val[ordinal_columns].astype(str).astype('category')\n#             test_scaled[ordinal_columns] = test_scaled[ordinal_columns].astype(str).astype('category')\n        \n#         fold_model = deepcopy(EstimatorMdl[est_id])\n#         fold_model.set_params(**params[est_id])\n#         if est_id in [1, 3]:\n#             fold_model.fit(X_tr, y_tr)\n#         elif est_id == 6:\n#             fold_model.fit(X_tr, y_tr, eval_set=(X_val, y_val), cat_features=ordinal_columns)\n#         else:\n#             fold_model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n    \n#         val_preds = fold_model.predict(X_val)\n#         oof_preds[est_id][val_idx] = val_preds\n#         final_test_preds[est_id] += fold_model.predict_proba(test_scaled)[:, 1] / skf.n_splits\n        \n#         fold_score = fold_model.score(X_val, y_val)\n#         print(f\"Fold {_id}  Score: {fold_score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # blending - before lgbm(.3) atfer lgbm(.7)\n# sub_lgbm = final_test_preds[5] * .7 + sub_dict['lgbm'] * .3\n# submission_lgbm = deepcopy(submission)\n# submission_lgbm['Heart Disease'] = sub_lgbm\n# submission_lgbm.to_csv(f'submission_sub_lgbm.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Score-based Weight w.lr\n# sub_SBW = 0.346 * final_test_preds[5] + 0.339 * final_test_preds[3] + 0.315 * final_test_preds[6]\n# submission_SBW = deepcopy(submission)\n# submission_SBW['Heart Disease'] = sub_SBW\n# submission_SBW.to_csv(f'submission_sub_SBW.csv', index=False)\n\n# # w LR\n# sub_SBWwLR = 0.326 * final_test_preds[5] + 0.323 * final_test_preds[3] + 0.301 * final_test_preds[6] + 0.05 * final_test_preds[1]\n# submission_SBWwLR = deepcopy(submission)\n# submission_SBWwLR['Heart Disease'] = sub_SBWwLR\n# submission_SBWwLR.to_csv(f'submission_sub_SBWwLR.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T11:59:31.261685Z","iopub.execute_input":"2026-02-22T11:59:31.261942Z","iopub.status.idle":"2026-02-22T11:59:31.267644Z","shell.execute_reply.started":"2026-02-22T11:59:31.261915Z","shell.execute_reply":"2026-02-22T11:59:31.266489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Simple Rank Weight w.lr\n# sub_SRW = 0.36 * final_test_preds[5] + 0.34 * final_test_preds[3] + 0.30 * final_test_preds[6]\n# submission_SRW = deepcopy(submission)\n# submission_SRW['Heart Disease'] = sub_SRW\n# submission_SRW.to_csv(f'submission_sub_SRW.csv', index=False)\n\n# # w LR\n# sub_SRWwLR = 0.35 * final_test_preds[5] + 0.32 * final_test_preds[3] + 0.28 * final_test_preds[6] + 0.05 * final_test_preds[1]\n# submission_SRWwLR = deepcopy(submission)\n# submission_SRWwLR['Heart Disease'] = sub_SRWwLR\n# submission_SRWwLR.to_csv(f'submission_sub_SRWwLR.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Top Tier Weight\n# sub_TTW = 0.60 * final_test_preds[5] + 0.20 * final_test_preds[3] + 0.20 * final_test_preds[6]\n# submission_TTW = deepcopy(submission)\n# submission_TTW['Heart Disease'] = sub_TTW\n# submission_TTW.to_csv(f'submission_sub_TTW.csv', index=False)\n\n# # w LR\n# sub_TTWwLR = 0.60 * final_test_preds[5] + 0.20 * final_test_preds[3] + 0.15 * final_test_preds[6] + 0.05 * final_test_preds[1]\n# submission_TTWwLR = deepcopy(submission)\n# submission_TTWwLR['Heart Disease'] = sub_TTWwLR\n# submission_TTWwLR.to_csv(f'submission_sub_TTWwLR.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # stack\n# X_meta = pd.DataFrame([oof_preds[5], oof_preds[3], oof_preds[6]]).T\n# y_meta = deepcopy(y)\n\n# meta_model = LogisticRegression(C=1.0, solver='lbfgs')\n# meta_model.fit(X_meta, y_meta)\n\n# X_test_meta = pd.DataFrame([final_test_preds[5], final_test_preds[3], final_test_preds[6]]).T\n# meta_test_prob = meta_model.predict_proba(X_test_meta)[:, 1]\n# submission_meta = deepcopy(submission)\n# submission_meta['Heart Disease'] = meta_test_prob\n# submission_meta.to_csv(f'submission_sub_stack.csv', index=False)\n\n# submission_metalr = deepcopy(submission)\n# submission_metalr['Heart Disease'] = 0.95 * meta_test_prob + .05 * final_test_preds[1]\n# submission_metalr.to_csv(f'submission_sub_stackwLR.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}